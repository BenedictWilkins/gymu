{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_pygame\n",
    "import torch\n",
    "import gym_mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import ray\n",
    "ray.init(num_gpus=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 10\n",
      "2 20\n",
      "3 30\n",
      "4 40\n",
      "5 50\n",
      "6 60\n",
      "7 70\n",
      "8 80\n",
      "9 90\n",
      "1 2\n"
     ]
    }
   ],
   "source": [
    "def gen():\n",
    "    for i in range(10):\n",
    "        yield i, i*10\n",
    "        \n",
    "for (i,j) in gen():\n",
    "    print(i,j)\n",
    "    \n",
    "def bar():\n",
    "    return 1,2\n",
    "\n",
    "(a,b) = bar()\n",
    "\n",
    "def foo((a,b)): #syntax error\n",
    "    print(a,b)\n",
    "    \n",
    "foo(1,2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyworld.toolkit.tools.visutils.jupyter as J\n",
    "\n",
    "\n",
    "env_f = lambda : gym_pygame.envs.Expander()\n",
    "\n",
    "\n",
    "lit = gym_mp.mp_episodes(env_f, workers=3, \n",
    "                          mode=gym_mp.mode.sa, n=1, \n",
    "                          max_length=1000)\n",
    "#lit = lit.for_each(lambda x: [torch.from_numpy(y) for y in x])\n",
    "for x in lit.gather_async():\n",
    "    s, a = x\n",
    "    print(s.shape)\n",
    "    J.images(s)\n",
    "    print(s[0][0], a.shape)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def use_gpu():\n",
    "    print(\"ray.get_gpu_ids(): {}\".format(ray.get_gpu_ids()))\n",
    "    print(\"CUDA_VISIBLE_DEVICES: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    \n",
    "use_gpu.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(ray.get_gpu_ids())\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_mp.iterators.episode('CartPole-v0', mode=gym_mp.mode.sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lit = gym_mp.mp_episodes('Pong-v0', workers=2, \n",
    "                            mode=gym_mp.mode.sa, n=1, \n",
    "                            max_length=1000)\n",
    "lit = lit.for_each(lambda x: [torch.from_numpy(y).cuda() for y in x]) #hmm... it seems complicated to enable cuda support -- might require a custom ParallelIteratorWorker\n",
    "\n",
    "for x in lit.gather_async():\n",
    "    s, a = x\n",
    "    s = s.cuda()\n",
    "    print(s[0][0], a.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TestEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TestEnv, self).__init__()\n",
    "        self.name = \"env\"\n",
    "        self.i = 0\n",
    "        self.n = 3\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.i += 1\n",
    "        time.sleep(np.random.randint(1000)/1000)\n",
    "        done = self.i > self.n\n",
    "        return \"{0}-{1}-{2}\".format(self.name, self.i, action), 0., done\n",
    "\n",
    "    def reset(self):\n",
    "        self.i = 0\n",
    "        return \"{0}-{1}-{2}\".format(self.name, self.i, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "policy_f = lambda : gym_mp.policy.uniform(gym.spaces.Discrete(2))\n",
    "\n",
    "env_f = TestEnv\n",
    "lit = gym_mp.async_iterator(env_f, policy_f, repeat=True)\n",
    "\n",
    "for s in lit:\n",
    "    print(*s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import ray.util.iter\n",
    "\n",
    "class wait_iter:\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.i = 0\n",
    "    \n",
    "    def __next__(self):\n",
    "        time.sleep(1)\n",
    "        self.i += 1\n",
    "        if self.i > self.n:\n",
    "            raise StopIteration()\n",
    "        return self.i\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "def gen(n):\n",
    "    for i in range(n):\n",
    "        time.sleep(1)\n",
    "        yield i\n",
    "\n",
    "class wait_iter2:\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return gen(self.n)\n",
    "        \n",
    "    \n",
    "it = ray.util.iter.from_iterators([range(10), wait_iter2(10)])\n",
    "\n",
    "lit = it.gather_async()\n",
    "\n",
    "for i in lit:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
